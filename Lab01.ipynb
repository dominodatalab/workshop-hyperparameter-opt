{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282e0c22-ab2d-45a9-902e-5809be1fd5ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter Optimisation Lab - 1\n",
    "\n",
    "## Building models with Gaussians\n",
    "\n",
    "In this section we'll consider using the Gaussian Distribution to model an arbitrary dataset.\n",
    "\n",
    "$$p(x | \\pi, \\Sigma) = (2\\pi)^{k/2}|\\Sigma|^{-1/2}\\text{exp}\\Bigl\\{ -\\frac{1}{2} (x-\\mu)^{\\prime} \\Sigma^{-1} (x-\\mu)\\Bigr\\}$$\n",
    "\n",
    "Why would we like to do this? There would not seem to be any gain in doing it, because normal distributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from a multivariate normal distribution is also normal:\n",
    "\n",
    "$$ p(x,y) = \\mathcal{N} \\biggl( \\begin{bmatrix}\n",
    "                                   \\mu_{x} \\\\\n",
    "                                   \\mu_{y} \\\\\n",
    "                                \\end{bmatrix},\n",
    "                                \\begin{bmatrix}\n",
    "                                   \\Sigma_x, \\Sigma_{xy} \\\\\n",
    "                                   \\Sigma_{xy}^T, \\Sigma_y \\\\\n",
    "                                \\end{bmatrix}\n",
    "\\biggr) $$\n",
    "\n",
    "$$\n",
    "    p(x) = \\int p(x,y) dy = \\mathcal{N} (\\mu_x, \\Sigma_x)\n",
    "$$\n",
    "\n",
    "Also, conditional distributions of a subset of the elements of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$\n",
    "p(x,y) = \\mathcal{N} (\\mu_x + \\Sigma_{xy} \\Sigma^{-1}_y (y - \\mu_y), \\Sigma_x - \\Sigma_{xy} \\Sigma^{-1}_y \\Sigma_{xy}^T)\n",
    "$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, with any marginal subset having a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an infinite vector is as a function. When we write a function that takes continuous values as inputs, we are essentially implying an infinite vector that only returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian represented as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a **distribution over functions**. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean function and a covariance function:\n",
    "\n",
    "$$p(x) \\sim \\text{GP} (m(x), k(x, y))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed.\n",
    "\n",
    "For example, one specification of a GP might be:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "m(x) &= 0 \\\\\n",
    "k(x, y) &= \\theta_1 \\text{exp} \\biggl( \\frac{\\theta_2}{2} (x-y)^2 \\biggr)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, the covariance function is a squared exponential, for which values of $x$ and $y$ that are close together result in values of $k$ closer to one, while those that are far apart return values closer to zero. It may seem odd to simply adopt the zero function to represent the mean function of the Gaussian process — surely we can do better than that! It turns out that most of the learning in the GP involves the covariance function and its hyperparameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean function and covariance function, respectively, evaluated at those points.\n",
    "\n",
    "## Sampling from a Gaussian Process\n",
    "\n",
    "To make this notion of a \"distribution over functions\" more concrete, let's quickly demonstrate how we obtain realizations from a Gaussian process, which results in an evaluation of a function over a set of points. All we will do here is a sample from the prior Gaussian process, so before any data have been introduced. What we need first is our covariance function, which will be the squared exponential, and a function to evaluate the covariance at given points (resulting in a covariance matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed004164-3665-4f8f-80b2-ddc6acaf5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def exponential_cov(x, y, theta):\n",
    "    return theta[0] * np.exp( -0.5 * theta[1] * np.subtract.outer(x, y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafec0a-3472-45ac-8488-779f33c2741f",
   "metadata": {},
   "source": [
    "We are going generate realizations sequentially, point by point, using the lovely conditioning property of mutlivariate Gaussian distributions. Here is that conditional:\n",
    "\n",
    "$$p(x,y) = \\mathcal{N} (\\mu_x + \\Sigma_{xy} \\Sigma^{-1}_y (y - \\mu_y), \\Sigma_x - \\Sigma_{xy} \\Sigma^{-1}_y \\Sigma_{xy}^T)$$\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Write a function *conditional(x_new, x, y, params)* that calculates $\\mu$ and $\\Sigma$ using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbb1ba-231b-42fe-a76a-8e2c0e9dbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, params):\n",
    "    \n",
    "    Sigma_xy = exponential_cov(x_new, x, params)\n",
    "    Sigma_y = exponential_cov(x, x, params)\n",
    "    Sigma_x = exponential_cov(x_new, x_new, params)\n",
    "    \n",
    "    mu = np.linalg.inv(Sigma_y).dot(Sigma_xy.T).T.dot(y)\n",
    "    sigma = Sigma_x - Sigma_xy.dot(np.linalg.inv(Sigma_y).dot(Sigma_xy.T))     \n",
    "    \n",
    "    return(mu.squeeze(), sigma.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6905d94-a0ab-48ed-944a-ac704a7e02eb",
   "metadata": {},
   "source": [
    "We will start with a Gaussian process prior with hyperparameters $\\sigma_0=1$, $\\sigma_2=10$. We will also assume a zero function as the mean, so we can plot a band that represents one standard deviation from the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b258f-dcf6-4f02-9630-64491ea8b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = [1, 10]\n",
    "σ_0 = exponential_cov(0, 0, θ)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=σ_0, capsize=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1db25-28c8-483f-bff5-1f18a20697d7",
   "metadata": {},
   "source": [
    "Let's select an arbitrary starting point to sample, say $x=1$. Since there are no previous points, we can sample from an unconditional Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d8c93-1bd6-4406-be4b-7731dd3e7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=σ_0)]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabea488-dcf1-49fe-9b0c-688c36a3716b",
   "metadata": {},
   "source": [
    "We can now update our confidence band, given the point that we just sampled, using the covariance function to generate new point-wise intervals, conditional on the value [$x_0$, $y_0$]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5e9e6-325d-4f3a-a2f3-31afec9a2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_1 = exponential_cov(x, x, θ)\n",
    "\n",
    "def predict(x, data, kernel, params, sigma, t):\n",
    "    k = [kernel(x, y, params) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, params) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new\n",
    "\n",
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_1, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049728ca-eb58-4459-bf9c-c58831c042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b524e-ca4c-4b14-85f9-00ed50a488ec",
   "metadata": {},
   "source": [
    "So conditional on this point, and the covariance structure we have specified, we have essentially constrained the probable location of additional points. Let's now sample another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffc8a8-c541-4a30-b1d8-7b5db6950844",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = conditional([-0.7], x, y, θ)\n",
    "y2 = np.random.normal(m, s)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7549ac5-6533-4b1f-bc64-2dd0a0d68c20",
   "metadata": {},
   "source": [
    "This point is added to the realization, and can be used to further update the location of the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadcf9f-ca90-4612-8e1f-7efc909b4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)\n",
    "σ_2 = exponential_cov(x, x, θ)\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_2, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9f09b-b86b-4052-b848-a34a4b94b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96f146-a0c3-4a98-9bcc-b2cb1450719e",
   "metadata": {},
   "source": [
    "Of course, sampling sequentially is just a heuristic to demonstrate how the covariance structure works. We can just as easily sample several points at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529604b-c2f1-4e82-b89a-31fa97f7f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y, θ)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "print(y_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bb9b0-a9ae-47b4-86b8-70a44c0b27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()\n",
    "σ_new = exponential_cov(x, x, θ)\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_new, y) for i in x_pred]\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc02f3-ded0-41aa-822d-acffa38f1236",
   "metadata": {},
   "source": [
    "So as the density of points becomes high, it results in a realization (sample function) from the prior GP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458dffd-d926-4d08-9470-a7577b3fade8",
   "metadata": {},
   "source": [
    "## Fitting Gaussian Processes using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693506a6-2373-4efc-a7fe-99470396f22c",
   "metadata": {},
   "source": [
    "Though it's entirely possible to extend the code above to introduce data and fit a Gaussian process by hand, there are a number of libraries available for specifying and fitting GP models in a more automated way. I will demonstrate and compare three packages that include classes and functions specifically tailored for GP modeling:\n",
    "\n",
    "* [scikit-learn](http://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "* [GPflow](https://github.com/GPflow/GPflow)\n",
    "* [PyMC3](https://docs.pymc.io/en/stable/)\n",
    "\n",
    "In particular, each of these packages includes a set of covariance functions that can be flexibly combined to adequately describe the patterns of non-linearity in the data, along with methods for fitting the parameters of the GP.\n",
    "\n",
    "Let's see how fitting a Gaussian process using scikit-learn works. First, let's generate some random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bf42b-d7b8-4984-971d-5ff09edd44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_vars( start=1, stop=10, step=1, mu=0, sigma=0, func=lambda x: x ):\n",
    "\n",
    "    # Generate x\n",
    "    x = np.arange(start, stop, step)  \n",
    "    \n",
    "    # Generate random noise\n",
    "    e = np.random.normal(mu, sigma, x.size)\n",
    "    \n",
    "    # Generate y values as y = func(x) + e\n",
    "    y = np.zeros(x.size)\n",
    "    \n",
    "    for ind in range(x.size):\n",
    "        y[ind] = func(x[ind]) + e[ind]\n",
    "    \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843d3d3-9d01-4b2b-92cb-e7bd4862c0b4",
   "metadata": {},
   "source": [
    "Notice, that we aren't adding noise to our *X* and *y* variables. Feel free to experiment later by injecting some noise (set *mu* and *sigma* to non-zero values) and see how the GP fitting process responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f35c75-1fdb-4b7d-b30c-b81b601a96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = corr_vars(-10, 10, .1, 0, 0, lambda x: 4*np.sin(x) + pow(0.5*x,2))\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(7,7))\n",
    "ax.scatter(X, y, color=\"blue\")       \n",
    "\n",
    "ax.set_title(r\"$y = 4 * sin(x) + (x/2)^2$\", fontsize=14)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edd203-cd75-4609-aa99-58666b7c458f",
   "metadata": {},
   "source": [
    "We have our data, now let's see what scikit-learn has to offer.\n",
    "\n",
    "For regression tasks, where we are predicting a continuous response variable, a [GaussianProcessRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html) is applied by specifying an appropriate covariance function, or **kernel**. Fitting proceeds by maximizing the log of the marginal likelihood, a convenient approach for Gaussian processes that avoids the computationally-intensive cross-validation strategy that is usually employed in choosing optimal hyperparameters for the model. The GaussianProcessRegressor does not allow for the specification of the mean function, always assuming it to be the zero function, highlighting the diminished role of the mean function in calculating the posterior.\n",
    "\n",
    "Now, let's sample some observations from our dataset and see how we can use them to model our data. The number of samples drawn from our dataset is controlled by the *sample_size* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787225f-6d63-40d8-89b5-78203872ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 3\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "training_indices = rng.choice(np.arange(y.size), size=sample_size, replace=False)\n",
    "\n",
    "X_train = X[training_indices]\n",
    "y_train = y[training_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250a4c3-e4a4-484e-9eba-f875774067b7",
   "metadata": {},
   "source": [
    "scikit-learn offers a library of about a dozen covariance functions, which they call kernels, to choose from.\n",
    "\n",
    "Here we will use a [Radial Basis Function kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html) (aka squared-exponential kernel). The RBF kernel is a stationary kernel. It is also known as the \"squared exponential\" kernel. It is parameterized by a length scale parameter $l>0$, which can either be a scalar or a vector with the same number of dimensions as the inputs X. The kernel is given by:\n",
    "\n",
    "$$k(x, y) = \\text{exp} \\biggl( -\\frac{d(x,y)^2}{2l^2} \\biggr) $$\n",
    " \n",
    "where $l$ is the length scale of the kernel and $d(.,.)$ is the Euclidean distance.\n",
    "\n",
    "Note, that the kernel hyperparameters are optimized during fitting unless the bounds are marked as \"fixed\".\n",
    "\n",
    "Your task here is to define an RBF kernel with $l=1.0$ with a lower and upper bound of $1×10^{-1}$ and $1×10^{2}$ respectively.\n",
    "\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Define the RBF kernel and fit the GP below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac27bc-e981-4546-91b9-ddaf8fef7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import gaussian_process\n",
    "\n",
    "# Define an RBF kernel\n",
    "kernel = gaussian_process.kernels.RBF(length_scale=2.0, length_scale_bounds=(1e-1, 1e2))\n",
    "\n",
    "# Set up a Gaussian process\n",
    "gp = gaussian_process.GaussianProcessRegressor(random_state=1234, kernel=kernel)\n",
    "\n",
    "# Fit the model\n",
    "gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c5c14-fb8f-4758-bc8b-a949f9aa7420",
   "metadata": {},
   "source": [
    "The fit method endows the returned model object with attributes associated with the fitting procedure; these attributes will all have an underscore (\\_) appended to their names. For example, the kernel_ attribute will return the kernel used to parameterize the GP, along with their corresponding optimal hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fc77b-1267-46e2-b9aa-1ac26dc4ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0262645-cc35-4cd2-ac2b-c33639d5f31f",
   "metadata": {},
   "source": [
    "Along with the fit method, each supervised learning class retains a predict method that generates predicted outcomes ($y^{\\ast}$) given a new set of predictors ($X^{\\ast}$) distinct from those used to fit the model. For a Gaussian process, this is fulfilled by the posterior predictive distribution, which is the Gaussian process with the mean and covariance functions updated to their posterior forms, after having been fit.\n",
    "\n",
    "$$ p(y^{\\ast}|y, x, x^{\\ast}) = \\text{GP} (m^{\\ast}(x^{\\ast}), k^{\\ast}(x^{\\ast})) $$\n",
    "\n",
    "Notice that we can calculate a prediction for arbitrary inputs $X^*$. To get a sense of the form of the posterior over a range of likely inputs, we can pass it a linear space as we have done above. *predict* optionally returns posterior standard deviations along with the expected value, so we can use this to plot a confidence region around the expected function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c323c-a60c-4ec5-acd7-14511b0270f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-10, 10, num=200).reshape(-1,1)\n",
    "y_pred, sigmas = gp.predict(x_pred, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359433-8aec-44e5-9393-9f565f678fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(X, y, label=r\"$f(x) = 4 * sin(x) + (x/2)^2$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, y_pred, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    x_pred.ravel(),\n",
    "    y_pred - 1.95 * sigmas,\n",
    "    y_pred + 1.95 * sigmas,\n",
    "    alpha=0.5,\n",
    "    label=\"95% confidence interval\"\n",
    ")\n",
    "plt.legend(loc = \"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c325e-2703-45d9-b915-ca56105c8430",
   "metadata": {},
   "source": [
    "### Task 3 [Optional]\n",
    "\n",
    "Try different values for *sample_size*, re-fit the model and observe how the predictions/confidence changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f8bb7-a925-45de-b1c5-a205d22424db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
