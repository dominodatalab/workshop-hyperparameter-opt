{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29c9447-7941-4632-8e52-c77a648d8c22",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation Lab - 0\n",
    "\n",
    "This notebook is a gentle introduction to hyperparameter tuning. In it we will familiarise ourselves with a simple convolutional neural network model that we will be fitting against a subset of [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "We start by importing all the necessary libraries for this tutorial.  We also set the random seed for the purposes of reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0bff9-c494-4e87-a813-0470f4fe02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import torchsummary\n",
    "import math \n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# This degrades the performance but enforces reproducibility.\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Note: This notebook hasn't been fully tested with GPU backends.\n",
    "# If you are using GPU-acceleration you may also have to enable the following lines:\n",
    "#\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.cuda.manual_seed_all(1234)\n",
    "\n",
    "# Make sure our tests are reproducible\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "  print(\"CUDA available. Using GPU acceleration.\\n\")\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  print(\"CUDA is NOT available. Using CPU for training.\\n\")\n",
    "  device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e2fb1-3338-43f8-a469-f4800ed36bc7",
   "metadata": {},
   "source": [
    "The next Python function loads the MNIST data set, shuffles the observations, and creates a training, validation, and test splits. It also creates Torch tensors using the resulting subsets so that we can feed this data directly to the CNN network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66069119-3448-43c0-8a88-1d26256de998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(device = \"cpu\", data_path = \"data\", train_fraction = 0.8, verbose = True):\n",
    "\n",
    "    assert 0.0 <= train_fraction <= 1.0\n",
    "    \n",
    "    train_X = np.load(os.path.join(data_path, \"train_X.npy\"))\n",
    "    train_y = np.load(os.path.join(data_path, \"train_y.npy\"))\n",
    "    test_X = np.load(os.path.join(data_path, \"test_X.npy\"))\n",
    "    test_y = np.load(os.path.join(data_path, \"test_y.npy\"))\n",
    "                     \n",
    "    indices = np.random.permutation(train_X.shape[0])\n",
    "    training_size = int(train_X.shape[0] * train_fraction)\n",
    "    training_idx, valid_idx = indices[:training_size], indices[training_size:]\n",
    "    train_X, valid_X = train_X[training_idx,:], train_X[valid_idx,:]\n",
    "    train_y, valid_y = train_y[training_idx], train_y[valid_idx]\n",
    "\n",
    "    train_X = torch.tensor(train_X, device=device).unsqueeze(dim=1).float()\n",
    "    train_y = torch.tensor(train_y, device=device)\n",
    "    valid_X = torch.tensor(valid_X, device=device).unsqueeze(dim=1).float()\n",
    "    valid_y = torch.tensor(valid_y, device=device)    \n",
    "    test_X = torch.tensor(test_X, device=device).unsqueeze(dim=1).float()\n",
    "    test_y = torch.tensor(test_y, device=device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"train_X size : \", train_X.shape)\n",
    "        print(\"train_y size : \", train_y.shape)\n",
    "        print(\"valid_X size : \", valid_X.shape)\n",
    "        print(\"valid_y size : \", valid_y.shape)\n",
    "        print(\"test_X size  : \", test_X.shape)\n",
    "        print(\"test_y size  : \", test_y.shape)\n",
    "        \n",
    "    return train_X, train_y, valid_X, valid_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b04ec-31f2-4f92-80c7-e2d71e00b45a",
   "metadata": {},
   "source": [
    "MNIST contains 70,000 samples (60,000 in the training and 10,000 in test subsets). Our *load_mnist()* function changes this to 48,000 in training, 12,000 in validation, and 10,000 in test. This, however, is still quite a lot of data. Given the time constraint of the workshop we'll further trim down the training subset to 20,000 samples in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf88dd-f57c-461c-bd5b-15213794c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, valid_X, valid_y, test_X, test_y = load_mnist(device, verbose=False)\n",
    "\n",
    "# Using only the first 20,000 training samples to save time at the workshop.\n",
    "# Comment out to use the complete MNIS training set\n",
    "train_X = train_X[:20000]\n",
    "train_y = train_y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714b726-c2b3-44ce-8ce7-7ccaac22bebc",
   "metadata": {},
   "source": [
    "We don't need to worry about class imbalance too much as the data has already been shuffled. We'll inspect the class distribution just in case to make sure that all 10 classes (digits 0-9) are relatively equally represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef7159-1363-4c5b-acd8-9396a22d0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_y.numpy(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a68d4b7-8f62-4f21-827f-9623876de0a3",
   "metadata": {},
   "source": [
    "We now define the architecture of the neural network that we'll be using for classifying the MNIST images. In this tutorial we'll use a [Convolutional Neural Network (CNN)](https://www.dominodatalab.com/blog/gpu-accelerated-convolutional-neural-networks-with-pytorch) via PyTorch. The feature extraction (or feature learning) part of the network employs a series of convolutional and pooling layers. \n",
    "\n",
    "* Each **convolutional layer** applies a filter, which is convolved with the input to create an activation map. The filter is slid over the image (both horizontally and vertically) and an output scalar is calculated for each spatial position. If the image contains data about colour, the typical approach is to process each colour channel separately, producing a tensor instead of a simple 2D matrix. \n",
    "\n",
    "* Convolutional layers generally preserve the dimensionality of the input. This may be problematic, because the same feature (e.g. an edge or a straight line) if present in different places in the image results in different feature maps. **Pooling layers** is what CNNs use to address this challenge. These layers downsample the image to lower resolutions in a way that also preserves the features present in the feature map.\n",
    "\n",
    "![Architecture of a CNN](images/cnn.png \"CNN architecture\")\n",
    "\n",
    "After the relevant features have been extracted, the final layer is flattened so that all features can be fed to the second component of the CNN - **a fully connected feedforward network**. This part of the network is responsible for performing the actual classification, and its number of outputs corresponds to the number of classes in the dataset (10 for MNIST). The outputs are typically evaluated after a *softmax* function, which is used to squash the raw scores into normalised values that add up to one. This is a standard configuration for a classical feedforward network tasked with classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62bf15-a73d-403c-9fde-b4386330489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, activ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.activ = activ\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,              \n",
    "                               out_channels=16,            \n",
    "                               kernel_size=5,             \n",
    "                               stride=1,                   \n",
    "                               padding=2)                            \n",
    "                \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)    \n",
    "        self.pool2 = nn.MaxPool2d(2)               \n",
    "        \n",
    "        self.fc = nn.Linear(32 * 7 * 7, 10)\n",
    "        \n",
    "        self.softmax = F.log_softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Our network has two pairs of convolutional/pooling layers for feature extraction\n",
    "        # Pair 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Pair 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Then the fully connected layer...\n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x = self.fc(x)\n",
    "    \n",
    "        #... and the softmax\n",
    "        output = self.softmax(x, dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def summary(self, input_size=(1, 28, 28)):\n",
    "        # Torch-summary provides information similar to Tensorflow's model.summary() API to view the visualization of the model.\n",
    "        # We'll use it to print information about the architecture and number of parameters of our network.\n",
    "        torchsummary.summary(self, input_size = input_size)\n",
    "        \n",
    "    def reset(self):\n",
    "        # This re-initializes the weights of the network without having to reinstate it\n",
    "        # We'll use it for performance and reproducibility purposes\n",
    "        for layer in self.children():\n",
    "           if hasattr(layer, \"reset_parameters\"):\n",
    "               layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6004cc-0f36-474d-815e-d198f3c2e875",
   "metadata": {},
   "source": [
    "Now let's build a training loop / method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aae6a-163f-48db-9411-c057016bc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, batch_size, loss_crierion, optimizer, train_X, train_y, max_epochs = 1, verbose = True, iterations = 1000):\n",
    "    \n",
    "    loss_hist = []\n",
    "\n",
    "    samples_per_epoch = train_X.shape[0]\n",
    "    \n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        # Loop over the number of mini-batched in each epoch\n",
    "        for index in range(0, samples_per_epoch, batch_size):\n",
    "\n",
    "            # Get the current mini-batch\n",
    "            images = train_X[index:index + batch_size, :, :]\n",
    "            labels = train_y[index:index + batch_size,]\n",
    "\n",
    "            # Set all gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Do a forward pass & calculate the loss\n",
    "            out = net(images)\n",
    "            loss = loss_crierion(out, labels)\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "            # Calculate the gradients via backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weight matrices\n",
    "            optimizer.step()\n",
    "            \n",
    "            if verbose and (index % iterations == 0):\n",
    "                 print(\"Epochs [{:d}/{:d}], Samples[{:d}/{:d}], Loss: {:.4f}\".format(epoch + 1, max_epochs, index + iterations, samples_per_epoch, loss))\n",
    "                    \n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef63f2-d46a-492b-9ccc-aaf3e1fecb18",
   "metadata": {},
   "source": [
    "Finally, let's create a function to calculate the accuracy using a holdout set (validation or test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8005c-b8a6-4b89-a2cc-97ff39cf2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_X, test_y):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Do a forward pass using the holdout data\n",
    "        out = net.forward(test_X)\n",
    "        # Convert the output probabilities to a one-hot vector\n",
    "        pred = torch.argmax(out, 1)\n",
    "        # Compare to ground truth\n",
    "        correct = pred.eq(test_y).sum()\n",
    "    \n",
    "    # Caclualte the accuracy\n",
    "    accuracy = correct / test_X.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea3dac-5bd8-4987-94f0-e0ae36a0c14d",
   "metadata": {},
   "source": [
    "### Task 1 - Test the network\n",
    "\n",
    "In this simple task you'll instantiate the CNN and run it with a set of manually selected hyperparameters. This is simply to ensure that our network functions properly and to help you familiarise yourself with the parameters that we'll be optimising.\n",
    "\n",
    "Use the cell below to create an instance of the network named *net*, which uses ReLU as its transfer function. Then print the Layer/Output/Params information of the network using its *summary()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d7ce7-57e1-4475-b251-b5787b7ff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network\n",
    "activ = F.relu\n",
    "\n",
    "net = Net(activ = activ)\n",
    "#net.to(device)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55c15f-b5a8-468f-bc23-a44199715330",
   "metadata": {},
   "source": [
    "Let's set some random hyperparameters and confirm that the network executes properly. It is generally a good idea to constrain the epochs to 1 to avoid long training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a069266-8b6a-4995-9a4f-9a57a773615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "momentum = 1.0\n",
    "max_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f2c9f-6d35-4353-9b45-128b5d5b4759",
   "metadata": {},
   "source": [
    "The final two bits of the puzzle are the loss function that the network will optimise and the optimiser used for adjusting the network's weights. As this is a classification problem we will use a criterion that computes the cross entropy loss. As for the optimiser, we'll start with the most simple option - stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695fc00-adcb-467e-929e-dd0e7cfe7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_crierion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fec97-4df1-4b6d-b4fe-d3ff89c6a4a4",
   "metadata": {},
   "source": [
    "Now let's call the *train()* method, passing the network, batch size, loss criterion, optimiser, and training data (X and y) as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283cf45-1e2c-4063-86df-5f63fbb8da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, batch_size, loss_crierion, optimizer, train_X, train_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcc047-cc09-4501-b285-74cab555cd16",
   "metadata": {},
   "source": [
    "If you've picked reasonable hyperparameters you should see the training loss decreasing with each iteration.\n",
    "\n",
    "Finally, let's test the performance of the network on the validation set (*valid_X* and *valid_y*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ab663-1d1f-4fad-bce3-125e906368f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call test()\n",
    "print(\"Testing...\")\n",
    "acc = test(net, valid_X, valid_y)\n",
    "print(\"Validation accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da27a03-9e3b-4561-bfb4-7fabe45628f9",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832261ac-a632-4a66-8de4-d5792acc8288",
   "metadata": {},
   "source": [
    "Now that we have a working network let's proceed with some rudimentary hyperparameter tuning.\n",
    "\n",
    "Let's start by defining ranges for the individual parameters. We'll put everything in a dictionary with the parameter names acting as keys and their respective test values stored in corresponding lists. We'll also add the loss and the optimiser as fixed parameters in case we want to try something different later on.\n",
    "\n",
    "Here is the definition of the parameters dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7539f-b756-4868-9d9d-49897cd7c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "          \"batch_size\"   : [10, 20, 50, 100],\n",
    "          \"momentum\"     : [0.9]}\n",
    "\n",
    "params[\"optimizer\"] = [optim.SGD]\n",
    "params[\"loss\"] = [nn.CrossEntropyLoss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556508b-149b-4029-b8a2-dc6f1b60910b",
   "metadata": {},
   "source": [
    "Our next task is to create a function *get_param_grid(params)*, which accepts the parameters dictionary and generates all possible parameter combinations. \n",
    "The function should return a list of dictionaries, each dictionary representing a single experiment. For example the first element of the list could be:\n",
    "\n",
    "```\n",
    "{'learning_rate': 0.1,\n",
    " 'batch_size': 10,\n",
    " 'momentum': 0.9,\n",
    " 'optimizer': torch.optim.sgd.SGD,\n",
    " 'loss': CrossEntropyLoss()}\n",
    "```\n",
    "\n",
    "The second element could be\n",
    "\n",
    "```\n",
    "{'learning_rate': 0.1,\n",
    " 'batch_size': 20,\n",
    " 'momentum': 0.9,\n",
    " 'optimizer': torch.optim.sgd.SGD,\n",
    " 'loss': CrossEntropyLoss()}\n",
    "```\n",
    "\n",
    " and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac1f08-45ad-478a-a5c2-bd660be58ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_param_grid(params):\n",
    "    \n",
    "    keys, values = zip(*params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62329110-96fa-47c1-a190-3b6267d98aba",
   "metadata": {},
   "source": [
    "Let's verify that your *get_param_grid(params)* implementation works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfb926-4a03-4146-b7c0-311bf745cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_param_grid(params)\n",
    "\n",
    "assert len(experiments) == math.prod(map(len, params.values())), \"It doesn't look like get_param_grid() generated the correct number of experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455933a3-e1a4-4bc5-b216-9e9576bd7e7b",
   "metadata": {},
   "source": [
    "We can also see a list of all experiemnts (or hyperparameter values) that we need to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7c2bf-ecb6-4c71-93a1-44eea82981ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*experiments,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fd3b9-438f-44de-aad2-85d353a8de8f",
   "metadata": {},
   "source": [
    "We now define a function to facilitate the execution of experiments. The function below, aptly named *run_experiment* takes an experiment from the list of experiments, a training set, and a holdout set. It then unpacks the hyperparameters, resets the network, calls *train()* with the specificed hyperparameters, and finally calculates the accuracy on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a626aa-3414-4767-a1cb-70586db39749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(exp, net, train_X, train_y, test_X, test_y):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if (exp[\"optimizer\"] == optim.SGD):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=exp[\"learning_rate\"], momentum=exp[\"momentum\"])\n",
    "    else:\n",
    "        optimizer = exp[\"optimizer\"](net.parameters(), lr=exp[\"learning_rate\"])\n",
    "\n",
    "    loss_crierion = exp[\"loss\"]\n",
    "    \n",
    "    # Make sure we aren't training an already trained network\n",
    "    net.reset()\n",
    "    \n",
    "    loss_hist = train(net, exp[\"batch_size\"], loss_crierion, optimizer, train_X, train_y, verbose=False)\n",
    "    \n",
    "    # Accuracy on holdout\n",
    "    acc = test(net, test_X, test_y)    \n",
    "    \n",
    "    # How long did training take?\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return loss_hist, acc, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6c204-e19a-44de-ac57-2f0124fe3c5a",
   "metadata": {},
   "source": [
    "Below we also set up a final helper function, which can traverse a list of experiments and their respecitve validation accuracy, and pick the best performing combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a48193-8774-4fbf-bc40-91c8841330f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_run(acc_hist, experiments, verbose=False):\n",
    "    \n",
    "    highest_acc = max(acc_hist)\n",
    "    highest_acc_idx = acc_hist.index(highest_acc)\n",
    "        \n",
    "    return highest_acc, experiments[highest_acc_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e9f29-f3a3-4f28-8069-03b8b3d90515",
   "metadata": {},
   "source": [
    "Here we have the main training loop. It will perform a comprehensive run of all experiments in the *experiments* list and pick the best performing combination using the *get_best_run()* function. The loop also calculates the ETA for the complete grid search as it progresses through the individual experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0feb9a7-7b7b-4b02-96f8-b74e66394229",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_counter = 0\n",
    "acc_hist = []\n",
    "\n",
    "runtime = 0\n",
    "\n",
    "for exp in experiments:   \n",
    "\n",
    "    exp_counter += 1\n",
    " \n",
    "    print(\"Exp {:d}/{:d}: \".format(exp_counter, len(experiments)), exp, end=\" \")\n",
    "    loss_hist, acc, elapsed_time = run_experiment(exp, net, train_X, train_y, test_X, test_y)\n",
    "    acc_hist.append(acc)\n",
    "    \n",
    "    runtime += elapsed_time\n",
    "    elapsed_time = str(timedelta(seconds = elapsed_time)).split(\".\")[0]\n",
    "    eta = str(timedelta(seconds = (runtime / exp_counter) * (len(experiments) - exp_counter))).split(\".\")[0]\n",
    "\n",
    "    print(\"Validation acc: {:.2f}\".format(acc), end=\" \")   \n",
    "    print(\"Runtime: {} ETA: {}\".format(elapsed_time, eta))\n",
    "    \n",
    "highest_acc, best_params = get_best_run(acc_hist, experiments)\n",
    "print(\"\\nHighest validation acc: {:.2f}\".format(highest_acc))\n",
    "print(\"Best parameters       :\", best_params)\n",
    "print(\"Total runtime         : \",  runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ada3b-d831-4be1-b20b-851a09377f00",
   "metadata": {},
   "source": [
    "### Task 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95442e67-f63d-40fb-92fe-258bba406741",
   "metadata": {},
   "source": [
    "In this taks we will expand the parameter space by adding more values to *momentum* and introducing a second optimiser *optim.Adagrad*.\n",
    "\n",
    "Let's first see how this impacts the number of parameters we need to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539965b-3668-4bba-948a-6d2091601c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "          \"batch_size\"   : [10, 20, 50, 100],\n",
    "          \"momentum\"     : [0.5, 0.9, 1.0]}\n",
    "\n",
    "params[\"optimizer\"] = [optim.SGD, optim.Adagrad]\n",
    "params[\"loss\"] = [nn.CrossEntropyLoss()]\n",
    "\n",
    "experiments = get_param_grid(params)\n",
    "\n",
    "len(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e9e58-deeb-44ae-80cb-bc4df9f0154a",
   "metadata": {},
   "source": [
    "This appears to be a substantial number of experiments. We need a different strategy if we don't want to wait too long for an exhaustive search of the highest validation accuracy. One simple idea is to employ random search.\n",
    "\n",
    "Your task is to write a function *get_random_grid(params, n)*, which takes the parameters dictionary and an integer *n* and produces *n* randomly generated experiments for our training loop to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f9ef4-351a-4090-ad13-00535201c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_grid(params, n):\n",
    "    \n",
    "    experiments = get_param_grid(params)\n",
    "    experiments = random.choices(experiments, k=n)\n",
    "    \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d4f0b-6293-4dcc-829e-4f9c6f55c82d",
   "metadata": {},
   "source": [
    "OK, now let's construct our *experiments* list. We'll use the same number of experiments as before (16), but this time the combinations of parameters have been randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d29992-7a2a-4b84-a4eb-ca53addcada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_random_grid(params, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2ea95-6be1-4ee3-8cfe-5865af2a4b82",
   "metadata": {},
   "source": [
    "Let's look at the experiments that we'll be trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f65627-21f7-4238-97d3-e180b6820c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*experiments,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0a17a-1362-4609-aa97-67ef03f67cf2",
   "metadata": {},
   "source": [
    "Now let's run the training loop as see how our random search perfoms compared to the naive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647e22d-b72b-45c0-8dfe-f4fba56343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_counter = 0\n",
    "acc_hist = []\n",
    "\n",
    "runtime = 0\n",
    "\n",
    "for exp in experiments:   \n",
    "\n",
    "    exp_counter += 1\n",
    " \n",
    "    print(\"Exp {:d}/{:d}: \".format(exp_counter, len(experiments)), exp, end=\" \")\n",
    "    loss_hist, acc, elapsed_time = run_experiment(exp, net, train_X, train_y, valid_X, valid_y)\n",
    "    acc_hist.append(acc)\n",
    "    \n",
    "    runtime += elapsed_time\n",
    "    elapsed_time = str(timedelta(seconds = elapsed_time)).split(\".\")[0]\n",
    "    eta = str(timedelta(seconds = (runtime / exp_counter) * (len(experiments) - exp_counter))).split(\".\")[0]\n",
    "\n",
    "    print(\"Validation acc: {:.2f}\".format(acc), end=\" \")   \n",
    "    print(\"Runtime: {} ETA: {}\".format(elapsed_time, eta))\n",
    "    \n",
    "highest_acc, best_params = get_best_run(acc_hist, experiments)\n",
    "print(\"\\nHighest validation acc: {:.2f}\".format(highest_acc))\n",
    "print(\"Best parameters       :\", best_params)\n",
    "print(\"Total runtime         : \",  runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ceff7-7259-448a-bd79-9f4cd2b532a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
