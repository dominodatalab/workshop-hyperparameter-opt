{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c3301d-c015-40ce-8a8f-0220c6839ad0",
   "metadata": {},
   "source": [
    "# Distributed Hyperparameter Tuning using Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a71f7-878d-4004-b248-1da8ab034fb1",
   "metadata": {},
   "source": [
    "This tutorial shows and example of using Ray Tune to perform a simple training task.  \n",
    "\n",
    "The task consists of training a CNN model on the MNIST dataset using PyTorch. As part of this hands-on we will integrate Ray Tune into a PyTorch training workflow, which is fairly straightforward. All we need to do besides building the model using standard PyTorch is to:\n",
    "\n",
    "1. wrap data loading and training in functions\n",
    "\n",
    "2. make some network parameters configurable\n",
    "\n",
    "3. add checkpointing (optional and not covered here)\n",
    "\n",
    "4. and define the search space for the model tuning\n",
    "\n",
    "See references for more information.\n",
    "\n",
    "#### References:\n",
    "\n",
    "Ray RLLib Documentation: [Ray RLLib Documentation](https://docs.ray.io/en/master/rllib.html)\n",
    "\n",
    "Ray Tune Documentation: [Ray Tune Documentation](https://docs.ray.io/en/master/tune/index.html)\n",
    "\n",
    "*This tutorial is adapted from the documentation for Ray version 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866115a-177d-435e-ad16-a7f161aa4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41fbbd-6632-46a6-888f-ac8746dd7b63",
   "metadata": {},
   "source": [
    "Let's begin by using the standard code for connecting to Ray (identical to what we did in Ray-Lab00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441efcd-f7f9-430a-be23-72e7f911287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized() == False:\n",
    "    print(\"Connecting to Ray cluster...\")\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce13ea6-071a-487f-8e59-aa5a34c56e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35bfc6-caa6-4823-965e-f30831d97ffd",
   "metadata": {},
   "source": [
    "Next, we define a very basic CNN with a one convolutional, one pooling, and one fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648f5dc-ffc2-40dc-b530-6637b884f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840ede9-dfe4-490c-8850-f6c1e48a22cb",
   "metadata": {},
   "source": [
    "Here things get a bit tricky. Because Ray uses a distributed architecture we can no longer store the training data in our local file system (i.e. in the workspace we are currently using). All Ray cluster nodes must have access to the data in order for Ray to be able to parallelize the execution. \n",
    "\n",
    "Luckily, Domino provides [Domion Datasets](https://docs.dominodatalab.com/en/latest/user_guide/0a8d11/domino-datasets/) - a high-performance, versioned, and structured filesystem. This filesystem is a collection of files that are available in user executions (e.g. workspaces and jobs) as a filesystem directory. Every time we start a workspace (with or without an attached cluster) Domino instantiates a local Dataset for the execution, which is available in */domino/datasets/local/\\<name_of_your_project\\>*. \n",
    "\n",
    "Because the name of your project forms part of the absolute dataset path, we can't hard-code it in the notebook. Instead, we'll use an environment variable containing your project name, which Domino conveniently sets for us in each execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56897e79-42b8-456c-9046-f662b0180f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domino_project = os.environ['DOMINO_PROJECT_NAME']\n",
    "data_path = os.path.join(\"/domino/datasets/local\", os.environ['DOMINO_PROJECT_NAME'])\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe44a5b-ea91-4a68-b471-8a020bd7c441",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "* Inspect the contents of *data_path*. You can open a new terminal by selecting File -> New -> Terminal in JupyterLab.\n",
    "\n",
    "Now let's get MNIST into the shared filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca97eb-d1da-4de8-9a58-d3dcac090aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset first\n",
    "datasets.MNIST(data_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb514cc-2c0b-4f18-a5b7-496c22f5c8f2",
   "metadata": {},
   "source": [
    "* Inspect *data_path* again. You should now have an MNIST directory created with the raw MNIST subsets (training, test) present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205e03f-7b84-4b8f-bb4a-48d6bcc0e216",
   "metadata": {},
   "source": [
    "Next, we set up our training loop. This is implemented in the *train_mist* function.\n",
    "\n",
    "Notice that *train_mnist* receives a config dictionary with the training hyperparameters. This is identical to how we were performing the training in HyperOpt. Inspect the code below and make sure you understand what the training loop is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70a506-cee4-4c99-870f-fa0620e4b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config):\n",
    "    \n",
    "    # Load the data into a PyTorch tensor and normalise it\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "    \n",
    "    # Load the training set\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(data_path, train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # Load the test set\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(data_path, train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "    # GPUs available?\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create an instance of the CNN model\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    # We use a fixed optim iser - SGD\n",
    "    # Notice that we get the learning rate and momentum from the config variable\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    \n",
    "    # Run 10 training iterations\n",
    "    for i in range(10):\n",
    "        train(model, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950972b7-b183-4960-b3d9-68c6c0bdd505",
   "metadata": {},
   "source": [
    "You noticed that the training loop calls two helper functions - *train* and *test* to update the model parameters and to report accuracy back to Tune. Here is their implementation.\n",
    "\n",
    "Notice, that we are again using a relatively small sample of the MNIST data, but feel free to tweak this if you'd like your training to run quicker or slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab97833-8d16-46da-8c9b-4fd9ce7e045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 1024\n",
    "TEST_SIZE = 256\n",
    "\n",
    "def train(model, optimizer, train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "            return\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b67d2-6233-40a1-a510-2b652de6a0ff",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Time to set up our search space.\n",
    "\n",
    "Tune provides a number of functions for sampling ranges in the search space. Some of the widely used sampling routines are:\n",
    "\n",
    "* *tune.uniform(a,b)* - Uniformly sample a float between a and b\n",
    "* *tune.randn(mu, std)* - Sample from a Gaussian defined by mean and standard deviation\n",
    "* *tune.sample_from(lambda x)* - Samples from a custom defined random function\n",
    "* *tune.grid_search([a,b,c])* - Performs a grid search over [a,b,c]\n",
    "* *tune.randint(a,b)* - Samples a random integer from [a,b]\n",
    "* *tune.choice([a,b,c])* - Sample uniformly from the list of values [a,b,c]\n",
    "\n",
    "For a comprehensive description of the random distributions API check [Ray Tune](https://docs.ray.io/en/releases-1.12.0/tune/api_docs/search_space.html)'s documentation.\n",
    "\n",
    "Now let's define a *search_space* dictionary with two hyperparameters --- learning rate and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46ce7b-1f0d-49f2-9983-d33aa70181d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.uniform(0.001, 0.1),\n",
    "    \"momentum\": tune.choice([0.1, 0.2, 0.5, 0.8, 0.9])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90549a1d-aeed-46b9-b69d-b4424bba0890",
   "metadata": {},
   "source": [
    "Next, we can run a single experiment to validate that our training loop/functions and hyperparameter space are set correctly.\n",
    "\n",
    "Ray Tune provides a function *tune.run* that executes experiments. Some of the key arguments of the function are:\n",
    "\n",
    "* *run_or_experiment* - training loop function (*train_mnist* in our case)\n",
    "* *config* - hyperparameter space. a sample of this dictionary will be passed to the training loop with each iteration\n",
    "* *resources_per_trial* - machine resources to allocate per trial, e.g. {\"cpu\": 64, \"gpu\": 8}\n",
    "* *checkpoint_freq* - how many training iterations between checkpoints\n",
    "* *scheduler* - TrialScheduler object for executing the experiment. For example, FIFO (default), MedianStopping, AsyncHyperBand, HyperBand etc.\n",
    "* *num_samples* â€“ Number of times to sample from the hyperparameter space. \n",
    "\n",
    "*tune.run* returns an *ExperimentAnalysis* instance for analyzing the results from a Tune experiment.\n",
    "\n",
    "Make a call to *tune.run* and execute 1 experiment using the current setup. Capture the results in a variable caled *analysis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1786f3-5ce6-4e60-ae10-c5396c5940b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(run_or_experiment = train_mnist, config = search_space, num_samples = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04763a69-e8e2-40a2-b0d6-f9119bbc9638",
   "metadata": {},
   "source": [
    "Let's look at the change in accuracy as traing progresses. We can get this information from the *analysis* object. We'll implement the plotting functionallity in a function as we'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0219d3-3892-4e77-a87f-19a279affb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(analysis, figsize=(8,6)):\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    dfs = analysis.trial_dataframes\n",
    "\n",
    "    # Plot by epoch\n",
    "    ax = None  # This plots everything on the same plot\n",
    "\n",
    "    trial_ids = analysis.results.keys()\n",
    "    for l, d in zip(trial_ids, dfs.values()):\n",
    "        ax = d.mean_accuracy.plot(ax=ax, legend=True, label=l)\n",
    "        \n",
    "plot_results(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c26ea9-440c-487f-a54d-a8388ae83dd9",
   "metadata": {},
   "source": [
    "The legend in the plot above provides the experiment id. We can see how the accuracy changes over the 10 epochs (remember the *i* loop in *train_mnist*)\n",
    "\n",
    "Now let's call *tune.run* again, but this time run 10 experiments. Note that Ray Tune first generates the various lr/momentum combinations that it will try, and then it executes the experiments in pairs (because Ray is currently limited to 2 single-CPU nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931e530-b261-41ca-9d7c-1e2c751d0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(run_or_experiment = train_mnist, config = search_space, num_samples = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156f5c2-3337-4128-aa42-8926d7a6c53d",
   "metadata": {},
   "source": [
    "Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801c742-e524-42ed-904a-6094f1a073c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d5b74-65db-475d-b4ab-7984f515623a",
   "metadata": {},
   "source": [
    "*ExperimentAnalysis* provides various methods for processing the experiment data. For example, *get_best_config* can show the configuration that provided the best result according to a specified criterion. In our case we care about the maximal accuracy so the call should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8874b32-f15a-420a-980a-cef8e4a8f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(\"mean_accuracy\", \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284f0aa-7c2f-4d64-824c-690ff4396663",
   "metadata": {},
   "source": [
    "We could also pull the results data into a data frame for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f5fcf-af9c-43c8-824a-c957a6741bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.trial_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06172c3-5949-4659-956d-c35c198777f4",
   "metadata": {},
   "source": [
    "And also look at detailed \"per epoch\" data for individual experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d3f3f-e4c4-41fa-80d5-f990a927befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfs.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb29a20-0d26-4bc3-9a96-6a75decb32cc",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "One issue with search that we've just conducted is that it keeps indiscriminately training all possible hyperparameter combinations for the fixed number of training iterations (epochs). This may not be the most optimal strategy, especially if we can identify that certain combinations are less promising. One obvious idea is to introduce some kind of an early stopping criterion, which terminates hyperparameter combinations that perform badly and focuses on more promising experiments. Ray Tune provides such functionallity in its ASHA (Asynchronous Successive HAlving) scheduler.\n",
    "\n",
    "The intuition behind the successive halving algorithm is the following. \n",
    "\n",
    "* We begin with all candidate configurations in a base rung.\n",
    "* Uniformly allocate a budget to a set of candidate hyperparameter configurations in a given rung.\n",
    "* Evaluate the performance of all candidate configurations.\n",
    "* Promote the top half of candidate configurations to the next rung.\n",
    "* Double the budget per configuration for the next rung and repeat until one configurations remains.  \n",
    "\n",
    "For more details on how ASHA works, please see [Massively Parallel Hyperparameter Optimization](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
    "\n",
    "Let's see how we can do the hyperparameter space exploration using ASHA. First, we create the scheduler and set the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2701917-2dd8-4be4-8e55-b5f52cd506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "asha = ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba2383-20ae-4ecb-8a18-f7e3dbd2117a",
   "metadata": {},
   "source": [
    "Now let's call *tune.run*. We can also crank up the number of samples to 20 as we'll be abandonig some of them anyway.\n",
    "Add the relevant values to the *tune.run* call and keep an eye on the *iter* column in the trials table that Ray Tune periodically prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc04fad-d43b-46f0-95fc-cc38706bc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    run_or_experiment = train_mnist, \n",
    "    config = search_space, \n",
    "    num_samples = 20,\n",
    "    scheduler = asha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c92992-f579-41d4-b1e8-7b455f5b5928",
   "metadata": {},
   "source": [
    "Despite doubling the number of experiment, you see that the total execution time only increased by about 20%.\n",
    "\n",
    "Now let's see the changes in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c77ddf-73e3-4d2f-a2e2-69a717cedf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(analysis, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9219f-63f8-4216-8719-6d84f25c5b13",
   "metadata": {},
   "source": [
    "You see that the majority of the trials were abandoned, with only a handful considered promising enough to continue training. Let's look at the best performing configuration. Use the *get_best_config* method on *analysis* to see this information. Does this match the result in the final experiments table above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d1daf-0963-43f2-aada-cf35d66cb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(\"mean_accuracy\", \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af6620-a42b-4a6a-92a1-c5675e38ead7",
   "metadata": {},
   "source": [
    "## HyperOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b495f3-b120-447d-922c-058cc8ab9038",
   "metadata": {},
   "source": [
    "So far we've looked at how Ray Tune can use various scheduling strategies to optimise the hyperparameter search. The flexibility of Tune, however, doesn't end there. It also allows us to plug external frameworks to perform the **selection** of hyperparameters for the individual experiments. As HyperOpt is one of the external frameworks supported by Tune, we could use some Bayesian reasoning in exploring the space. In this way we can incorporate previous knowledge in the process and also actively explore more promising regions of space.\n",
    "\n",
    "In addition, Ray Tune also supports the following other search algorithms:\n",
    "\n",
    "* [Ax](https://ax.dev/)\n",
    "* [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "* [HpBandSter](https://github.com/automl/HpBandSter)\n",
    "* [FAML](https://github.com/microsoft/FLAML)\n",
    "* [scikit-optimize](https://scikit-optimize.github.io/stable/)\n",
    "* ...\n",
    "* and many more\n",
    "\n",
    "Let's see the HyperOpt/Ray Tune integration in action. First we need some extra libraries (HyperOpt included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f38b81-7d33-4654-8e6d-f5517c6bbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a577e56-ee2e-4067-b948-8fad81fb6022",
   "metadata": {},
   "source": [
    "Define the search space (note that we are using *hp.* functions for the hyperparameter sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee05be-993a-4fd1-87f7-e0e2adea5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"lr\": hp.uniform(\"learning_rate\", 0.0001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab9241-8d45-44a6-8714-4e93a54f1878",
   "metadata": {},
   "source": [
    "Now let's wrap hyperopt in the HyperOptSearch object provided by tune. The arguments are pretty self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466e8cc-41c6-487f-804c-305ef4bffeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_search = HyperOptSearch(space, metric=\"mean_accuracy\", mode=\"max\", random_state_seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3f627-1334-409e-a8e4-1d3c2ec31efc",
   "metadata": {},
   "source": [
    "The only argument we need to provide this time is the training loop and *search_alg*. There is no need to specify the search space as this has already been provided to the *HyperOptSerach* object and is handled externally. \n",
    "\n",
    "Now set the search algorithm in the *tune.run* call and run the search.\n",
    "\n",
    "**Optional**: If you are feeling adventurous switch to the ASHA scheduler and increase the number of experiments to 20. Do you observe a difference in the behaviour of Ray Tune when scheduling experiments? How does it differ from the previous ASHA run? Why does it behave this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7df0a-f628-4dae-b639-9aec69be48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    run_or_experiment = train_mnist, \n",
    "    search_alg = hyperopt_search,\n",
    "    num_samples = 20,\n",
    "    scheduler=asha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2b78e-077f-4fd2-8134-04628b568c0f",
   "metadata": {},
   "source": [
    "Now let's see the accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d6d0c-4bba-4ab0-b49b-c736eefb360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(analysis, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b60439-ba0a-4b01-b97b-1875a22f32df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
